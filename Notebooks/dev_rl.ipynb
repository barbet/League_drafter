{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dev_rl","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPM41HOH8zLPh7mMUySSqLU"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"3YoNcmiYPphM","executionInfo":{"status":"ok","timestamp":1605366729507,"user_tz":-60,"elapsed":21073,"user":{"displayName":"tristan barbe","photoUrl":"","userId":"11052037289143514433"}},"outputId":"1ba12bb3-ba35-42ef-a887-f5670b87c2eb","colab":{"base_uri":"https://localhost:8080/"}},"source":["# if on google collab\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","sys.path.append('/content/drive/My Drive/Lol/models')\n","import c2v\n","import winner\n","\n","data_path = '/content/drive/My Drive/Lol/data'\n","model_path = '/content/drive/My Drive/Lol/models'\n","c2v_name = 'c2v5'\n","experiment_name = 'winner_with_both_path2'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_rbNE6LDcHDq","executionInfo":{"status":"ok","timestamp":1605449828688,"user_tz":-60,"elapsed":3254,"user":{"displayName":"tristan barbe","photoUrl":"","userId":"11052037289143514433"}}},"source":["import numpy as np\n","import logging\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.distributions import Categorical\n","from abc import ABC"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"JqdTAUmmQ_yF","executionInfo":{"status":"ok","timestamp":1605449830122,"user_tz":-60,"elapsed":767,"user":{"displayName":"tristan barbe","photoUrl":"","userId":"11052037289143514433"}}},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qMsaRtL5SRwH"},"source":["## memory"]},{"cell_type":"code","metadata":{"id":"5D4PfcOvQ6ob","executionInfo":{"status":"ok","timestamp":1605449798316,"user_tz":-60,"elapsed":624,"user":{"displayName":"tristan barbe","photoUrl":"","userId":"11052037289143514433"}}},"source":["class Memory:\n","    def __init__(self):\n","        self.actions = []\n","        self.states = []\n","        self.logprobs = []\n","        self.rewards = []\n","        self.is_terminals = []\n","    \n","    def clear_memory(self):\n","        del self.actions[:]\n","        del self.states[:]\n","        del self.logprobs[:]\n","        del self.rewards[:]\n","        del self.is_terminals[:]"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PlADh0SlSUXb"},"source":["## env"]},{"cell_type":"code","metadata":{"id":"sy2o53ngUDhj","executionInfo":{"status":"ok","timestamp":1605470531760,"user_tz":-60,"elapsed":758,"user":{"displayName":"tristan barbe","photoUrl":"","userId":"11052037289143514433"}}},"source":["class MatchMaker:\n","  def __init__(self, agent1, agent2, action_dim, judge):\n","    self.agent1 = agent1\n","    self.agent2 = agent2\n","    self.action_dim = action_dim\n","    self.turns_teams = [1,-1,1,-1,1,-1, 1, -1, -1, 1 ,1 ,-1, -1, 1, -1, 1, -1, 1, 1, -1]\n","    self.turns_action = ['B'] * 6 + ['P']*6 + ['B']*4 + ['P']*4\n","    self.judge = judge\n","    self.reset()\n","\n","\n","  def reset(self):\n","    self.state = np.zeros(shape = (20, self.action_dim))\n","    self.agent1.reset()\n","    self.agent2.reset()\n","\n","    # 1: start, -1: end\n","    self.starter = np.random.choice([-1, 1])\n","    self.agent1.set_side(self.starter)\n","    self.agent2.set_side(-self.starter)\n","    self.agents = {\n","        1: [self.agent1, self.agent2][int((self.starter+1)/2)],\n","        -1: [self.agent1, self.agent2][int((-self.starter+1)/2)]\n","      }\n","    self.turn = 0\n","    self.available_champions = [_ for _ in np.arange(action_dim)]\n","\n","    return self.state\n","\n","  def step(self):\n","    \"\"\"\n","    /!\\ not gym api like\n","    \"\"\"\n","    if self.turn > len(self.turns_teams):\n","      return \n","\n","    # set up turn variables\n","    current_team = self.turns_teams[self.turn]\n","    current_action_type = self.turns_action[self.turn]\n","    current_agent = self.agents[current_team]\n","\n","    # store state\n","    current_agent.memorize_state(self.state)\n","\n","    # get action and check/update action availability\n","    action, logprob = current_agent.next_action(self.state, current_action_type)\n","    assert(action in self.available_champions)\n","    self.available_champions.remove(champion)\n","    self.agents[-current_team].remove_choice(action)\n","\n","    # one hot champion ID to update env state\n","    state_turn = np.zeros(self.action_dim)\n","    state_turn[action] = 1\n","    self.state[self.turn, :] = state_turn\n","\n","    self.turn += 1\n","    done, reward = self.find_winner_proba()\n","\n","\n","    # store remaining info\n","    current_agent.memorize_reste(action, logprob, reward, done)\n","    # correct done and reawrd info if over\n","    if done:\n","      self.agents[1].memory.rewards[-1] = reward\n","      self.agents[1].memory.is_terminals[-1] = done\n","      \n","      self.agents[-1].memory.rewards[-1] = -reward\n","      self.agents[-1].memory.is_terminals[-1] = done\n","\n","    return \n","\n","\n","  def find_winner_proba(self):\n","    if self.turn > len(self.turns_teams):\n","      blue_team = self.state[[7,10,11,18,19]].sum(axis=0)\n","      res_team = self.state[[8,9,12,17,20]].sum(axis=0)\n","      return 1, judge.predict([blue_team, red_team])\n","    else:\n","      return 0, 0\n","\n","\n","  def learn(self, max_episodes, update_timestep, update_episode):\n","    running_reward = 0\n","    avg_length = 0\n","    for i_episode in range(1, max_episodes+1):\n","        state = env.reset()\n","        for t in range(len(turns_teams)):\n","\n","            self.step()\n","            \n","\n","            \n","        # update if its time\n","        if i_episode % update_episode == 0:\n","            for key, agent in self.agents.items():\n","              agent.train()\n","       \n","        # stop training if avg_reward > solved_reward\n","        if running_reward > (log_interval*solved_reward):\n","            print(\"########## Solved! ##########\")\n","            torch.save(ppo.policy.state_dict(), './PPO_{}.pth'.format(env_name))\n","            break\n","            \n","        # logging\n","        if i_episode % log_interval == 0:\n","            avg_length = int(avg_length/log_interval)\n","            running_reward = int((running_reward/log_interval))\n","            \n","            print('Episode {} \\t avg length: {} \\t reward: {}'.format(i_episode, avg_length, running_reward))\n","            running_reward = 0\n","            avg_length = 0"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"anEf21jdSWIM"},"source":["## agent"]},{"cell_type":"code","metadata":{"id":"lTwlfdo8UDkb"},"source":["class DraftAgent:\n","  def __init__(self, action_dim):\n","    self.side = None\n","    self.action_dim = action_dim\n","    self.memory = Memory()\n","    self.reset()\n","\n","  def reset(self):\n","    self.possible_choice = [_ for _ in np.arange(action_dim)]\n","    self.composition = []\n","    \n","\n","  def remove_choice(self, champion):\n","    self.possible_choice.remove(champion)\n","\n","  @abstractmethod\n","  def _internal_logic(self, state, action_type):\n","    pass\n","\n","  def train(self):\n","    self.memory.clear_memory()\n","    pass\n","\n","  def next_action(self, state, action_type):\n","    \"\"\"\n","    return a champion ID\n","    \"\"\" \n","    champion_chosen, logprob = self._internal_logic(state, action_type)\n","    self.remove_choice(champion_chosen)\n","    if action_type = 'P':\n","      self.composition.append(champion_chosen)\n","    return champion_chosen, logprob\n","\n","  def set_side(self, side):\n","    self.side=side\n","\n","  def memorize_state(self,  state):\n","    self.memory.states.append(state)\n","\n","  def memorize_reste(self, action, logprob, reward, done):\n","    self.memory.actions.append(action)\n","    self.memory.logprobs.append(logprob)\n","    self.memory.rewards.append(reward)\n","    self.memory.is_terminals.append(done)\n","\n","def RandomAgent(DraftAgent):\n","  def _internal_logic(self, state):\n","    return np.random.choice(self.possible_choice), 0\n","\n","def PPOAgent(DraftAgent):\n","  def __init__(self, action_dim, ppo):\n","    super().__init__(self, action_dim)\n","    self.ppo = ppo\n","\n","  def _internal_logic(self, state, aciton_type):\n","                # Running policy_old:\n","            action = ppo.policy_old.act(state, memory)\n","            state, reward, done, _ = env.step(action)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YOsEq__4gFmG","executionInfo":{"status":"ok","timestamp":1605471027256,"user_tz":-60,"elapsed":565,"user":{"displayName":"tristan barbe","photoUrl":"","userId":"11052037289143514433"}}},"source":["class ActorCritic(nn.Module):\n","    \"\"\"\n","    from https://github.com/nikhilbarhate99/PPO-PyTorch/blob/master/test_continuous.py\n","    \"\"\"\n","    def __init__(self, state_dim, action_dim, n_latent_var):\n","        super(ActorCritic, self).__init__()\n","\n","        # actor\n","        self.action_layer = nn.Sequential(\n","                nn.Linear(state_dim, n_latent_var),\n","                nn.Tanh(),\n","                nn.Linear(n_latent_var, n_latent_var),\n","                nn.Tanh(),\n","                nn.Linear(n_latent_var, action_dim),\n","                nn.Softmax(dim=-1)\n","                )\n","        \n","        # critic\n","        self.value_layer = nn.Sequential(\n","                nn.Linear(state_dim, n_latent_var),\n","                nn.Tanh(),\n","                nn.Linear(n_latent_var, n_latent_var),\n","                nn.Tanh(),\n","                nn.Linear(n_latent_var, 1)\n","                )\n","        \n","    def forward(self):\n","        raise NotImplementedError\n","        \n","    def act(self, state, memory):\n","        state = torch.from_numpy(state).float().to(device) \n","        action_probs = self.action_layer(state)\n","        dist = Categorical(action_probs)\n","        action = dist.sample()\n","        \n","        memory.states.append(state)\n","        memory.actions.append(action)\n","        memory.logprobs.append(dist.log_prob(action))\n","        \n","        return action.item()\n","    \n","    def evaluate(self, state, action):\n","        action_probs = self.action_layer(state)\n","        dist = Categorical(action_probs)\n","        \n","        action_logprobs = dist.log_prob(action)\n","        dist_entropy = dist.entropy()\n","        \n","        state_value = self.value_layer(state)\n","        \n","        return action_logprobs, torch.squeeze(state_value), dist_entropy\n","        \n","class PPO:\n","    def __init__(self, state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip):\n","        self.lr = lr\n","        self.betas = betas\n","        self.gamma = gamma\n","        self.eps_clip = eps_clip\n","        self.K_epochs = K_epochs\n","        \n","        self.policy = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n","        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n","        self.policy_old = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n","        self.policy_old.load_state_dict(self.policy.state_dict())\n","        \n","        self.MseLoss = nn.MSELoss()\n","    \n","    def update(self, memory):   \n","        # Monte Carlo estimate of state rewards:\n","        rewards = []\n","        discounted_reward = 0\n","        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n","            if is_terminal:\n","                discounted_reward = 0\n","            discounted_reward = reward + (self.gamma * discounted_reward)\n","            rewards.insert(0, discounted_reward)\n","        \n","        # Normalizing the rewards:\n","        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n","        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n","        \n","        # convert list to tensor\n","        old_states = torch.stack(memory.states).to(device).detach()\n","        old_actions = torch.stack(memory.actions).to(device).detach()\n","        old_logprobs = torch.stack(memory.logprobs).to(device).detach()\n","        \n","        # Optimize policy for K epochs:\n","        for _ in range(self.K_epochs):\n","            # Evaluating old actions and values :\n","            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n","            \n","            # Finding the ratio (pi_theta / pi_theta__old):\n","            ratios = torch.exp(logprobs - old_logprobs.detach())\n","                \n","            # Finding Surrogate Loss:\n","            advantages = rewards - state_values.detach()\n","            surr1 = ratios * advantages\n","            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n","            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n","            \n","            # take gradient step\n","            self.optimizer.zero_grad()\n","            loss.mean().backward()\n","            self.optimizer.step()\n","        \n","        # Copy new weights into old policy:\n","        self.policy_old.load_state_dict(self.policy.state_dict())"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"jst7DAK9SE0V","executionInfo":{"status":"ok","timestamp":1605471021711,"user_tz":-60,"elapsed":574,"user":{"displayName":"tristan barbe","photoUrl":"","userId":"11052037289143514433"}}},"source":[""],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"8aVlZaETUDnE"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8uibpfSsUDps"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cooTY-AfPwnc"},"source":[""],"execution_count":null,"outputs":[]}]}